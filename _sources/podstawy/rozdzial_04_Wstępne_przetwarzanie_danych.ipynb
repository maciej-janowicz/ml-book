{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN31e3wFck6ZwplUnS4U5JQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zWHlrZk7U8bk"},"outputs":[],"source":["def czyszczenie_danych(df):\n","    # Usuwanie podwojonych wierszy\n","    df.drop_duplicates(inplace = True)\n","    #\n","    # Usuwanie części kolumn o charakterze identyfikacyjnym\n","    df.drop(df[['Unnamed: 0','ID_ZLA', 'ID_PL', 'ID_UB', 'ID_LEKARZ', 'ID_ZOZ', 'STAN_NA',\n","               'O_PAR01_STAN_NA']], axis = 1, inplace = True)\n","    # Określenie liczby kolumn z jedną wartością:\n","    liczba_kolumn_z_1_wartością = df.nunique()\n","    # Określenie kolumn do usunięcia\n","    do_usunięcia = [i for (i, v) in enumerate(liczba_kolumn_z_1_wartością) if v == 1]\n","    # Usunięcie kolumn z jedną wartością:\n","    df.drop(df.columns[do_usunięcia], axis = 1, inplace = True)\n","    #\n","    # Wstawianie brakujących wartości typu float\n","    imputer = SimpleImputer(strategy = 'mean')\n","    df1 = df.select_dtypes('float64')\n","    df_imp1  = pd.DataFrame(imputer.fit_transform(df1), columns = df1.columns)\n","    # Wstawianie brakujących wartości typu int64\n","    df2 = df.select_dtypes('int64')\n","    imputer = SimpleImputer(strategy ='most_frequent')\n","    df_imp2 = pd.DataFrame(imputer.fit_transform(df2), columns = df2.columns)\n","    # Wstawianie brakujących wartości typu object\n","    df3 = df.select_dtypes('object')\n","    imputer = SimpleImputer(strategy = 'constant', fill_value = 'NIEZNANE')\n","    df_imp3 = pd.DataFrame(imputer.fit_transform(df3), columns = df3.columns)\n","    df = pd.concat([df_imp1, df_imp2, df_imp3], axis = 1)\n","    # Usuwanie wartości odstających z kolumn typu float64\n","    lof = LocalOutlierFactor()\n","    mniam = lof.fit_predict(df_imp1)\n","    maska = mniam != -1\n","    df_pcz = df[maska]\n","    print(\"Postacie ramek danych w funkcji czyszczenie\")\n","    print(df1.shape, df_imp1.shape)\n","    print(df2.shape, df_imp2.shape)\n","    print(df3.shape, df_imp3.shape)\n","    return df_pcz"]},{"cell_type":"code","source":["def czyszczenie_danych_ulepszone(df):\n","    \"\"\"Czyszczenie ramki danych, występującej jako parametr formalny.\n","\n","    Argumenty:\n","        df: obiekt pandas DataFrame, który ma być oczyszczony.\n","\n","    Zwraca:\n","        oczyszczoną ramkę danych.\n","    \"\"\"\n","\n","    # 1. Usuwanie jednego z identycznych wierszy:\n","    df.drop_duplicates(inplace=True)\n","\n","    # 2. Usuwanie niepotrzebnej kolumny z identyfikatorami oraz datami i czasem:\n","    #   Utworzenie listy kolumn do usunięcia i ich usuwanie\n","    kolumny_do_usunięcia = ['Unnamed: 0', 'ID_ZLA', 'ID_PL', 'ID_UB', 'ID_LEKARZ',\n","                         'ID_ZOZ', 'STAN_NA', 'O_PAR01_STAN_NA', 'DATA_WP']\n","    df.drop(columns=kolumny_do_usunięcia, errors='ignore', inplace=True)\n","\n","    # 3. Usuwanie kolumn, zawierających tylko jedną wartość:\n","    df = df.loc[:, df.nunique() > 1]\n","\n","    # 4. Wstawianie brakujących wartości przy użyciu modułu 'pipeline':\n","    #   - Użycie klasy sklearn.pipeline.Pipeline zapewnia lepszą organizację\n","    #   - Strategie wstawiania są różne w zależności od typu kolumny\n","    #\n","    # 'Pipeline' jest narzędziem, które pozwala na łączenie różnych kroków\n","    # przetwarzania danych i modelowania w jeden, spójny proces.\n","    # Pipeline umożliwia automatyzację i uporządkowanie kolejnych etapów przetwarzania\n","    # danych, takich jak przetwarzanie wstępne (np. skalowanie, kodowanie),\n","    # selekcja cech i właściwe modelowanie.\n","\n","    # Główne zalety użycia Pipeline w scikit-learn to:\n","\n","    # 1. Uproszczenie kodu: Pipeline pozwala na zdefiniowanie całego procesu\n","    # przetwarzania i modelowania w jednym miejscu, co czyni kod bardziej\n","    # czytelnym i łatwym do zarządzania.\n","    # 2. Unikanie błędów: Automatyzacja kolejnych kroków przetwarzania zmniejsza\n","    # ryzyko popełnienia błędów, takich jak pominięcie ważnego etapu.\n","    # Łatwe walidowanie krzyżowe: Pipeline może być łatwo użyty w walidacji krzyżowej,\n","    # co ułatwia ocenę modelu i pozwala uniknąć wycieku danych (\"data leakage\").\n","\n","    from sklearn.pipeline import Pipeline\n","\n","\n","    # SimpleImputer jest klasą w module sklearn.impute biblioteki scikit-learn,\n","    # która służy do uzupełniania brakujących wartości (missing values) w zbiorze danych.\n","    # Uzupełnianie brakujących wartości jest ważnym krokiem w przygotowaniu danych\n","    # do analizy i modelowania, ponieważ wiele algorytmów uczenia maszynowego\n","    # nie radzi sobie z brakującymi danymi.\n","\n","    # SimpleImputer oferuje kilka strategii uzupełniania brakujących wartości:\n","    # Średnia (mean): Uzupełnianie brakujących wartości średnią z kolumny.\n","    # Mediana (median): Uzupełnianie brakujących wartości medianą z kolumny.\n","    # Najczęstsza wartość (most_frequent): Uzupełnianie brakujących wartości najczęściej występującą wartością w kolumnie.\n","    # Stała wartość (constant): Uzupełnianie brakujących wartości określoną stałą wartością\n","\n","    from sklearn.impute import SimpleImputer\n","\n","\n","    # ColumnTransformer jest klasą w module sklearn.compose biblioteki scikit-learn,\n","    # która pozwala na stosowanie różnych transformacji do różnych kolumn w zbiorze danych.\n","    # Jest to przydatne, gdy różne kolumny wymagają różnych rodzajów przetwarzania,\n","    # takich jak skalowanie, kodowanie kategorialnych zmiennych lub uzupełnianie\n","    # brakujących wartości.\n","\n","    # ColumnTransformer umożliwia zdefiniowanie różnych transformacji dla różnych kolumn\n","    # i łączy je w jeden, spójny proces. Można określić, które kolumny mają być\n","    # przetwarzane przez które transformacje, co czyni ColumnTransformer\n","    # bardzo elastycznym narzędziem do przygotowania danych.\n","\n","    from sklearn.compose import ColumnTransformer\n","\n","    cechy_numeryczne = df.select_dtypes(include=['number']).columns\n","    cechy_kategorialne = df.select_dtypes(include=['object']).columns\n","\n","    potok_numeryczny = Pipeline([\n","        ('imputer', SimpleImputer(strategy='mean')),\n","    ])\n","\n","    potok_kategorialny = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='NIEZNANE')),\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', potok_numeryczny, cechy_numeryczne),\n","            ('cat', potok_kategorialny, cechy_kategorialne),\n","        ])\n","\n","    df = pd.DataFrame(preprocessor.fit_transform(df),\n","                      columns=cechy_numeryczne.tolist() + cechy_kategorialne.tolist(),\n","                      index=df.index)  # Keep original index\n","\n","    # 5. Usuwanie wartości odstających za pomocą LocalOutlierFactor:\n","    #   - Stosuje się tylko do cech numerycznych\n","\n","\n","\n","    # LocalOutlierFactor (LOF) jest algorytmem wykrywania anomalii (outliers)\n","    # zawartym w module sklearn.neighbors biblioteki scikit-learn.\n","    # Algorytm LOF służy do identyfikowania punktów danych, które są \"oddalone\"\n","    # od reszty danych, co oznacza, że są rzadkie lub niezwykłe w porównaniu\n","    # z resztą zbioru danych.\n","\n","    # LOF działa na zasadzie porównywania gęstości lokalnej punktów danych.\n","    # Punkty, które mają niższą gęstość lokalną w porównaniu z ich sąsiadami,\n","    # są uznawane za anomalie. Algorytm LOF jest szczególnie przydatny w przypadkach,\n","    # anomalie nie są jednoznacznie oddalone od reszty danych,\n","    # ale są rzadkie w lokalnym otoczeniu.\n","\n","    # Główne parametry LocalOutlierFactor to:\n","\n","    # n_neighbors: Liczba sąsiadów do rozważenia przy obliczaniu gęstości lokalnej.\n","    # contamination: Proporcja anomalii w zbiorze danych. Używana do ustalenia progu decyzyjnego.\n","    # novelty: Jeśli True, algorytm jest używany do wykrywania anomalii w nowych danych (novelty detection). Jeśli False, algorytm jest używany do wykrywania anomalii w danych treningowych.\n","    # Metoda fit_predict zwraca etykiety, gdzie -1 oznacza anomalie, a 1 oznacza normalne punkty.\n","\n","    from sklearn.neighbors import LocalOutlierFactor\n","\n","    lof = LocalOutlierFactor()\n","    outlier_mask = lof.fit_predict(df[cechy_numeryczne]) != -1\n","    df = df[outlier_mask]\n","\n","    return df"],"metadata":{"id":"7ufw00lIbrzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def przygotowanie_danych(df_prim):\n","    print(\"hihi\")\n","    dane_wyj = czyszczenie_danych(df_prim)\n","    print(\"Postać danych dane_wyj\")\n","    print(dane_wyj.shape)\n","    # Kodowanie danych typu 'int' oraz 'object'\n","    X, y = dane_wyj.drop(columns = 'fraud_wyk'), dane_wyj[['fraud_wyk']]\n","    print(type(X), X.shape, type(y), y.shape)\n","    Xfloat = X.select_dtypes('float64')\n","    Xkat = X.select_dtypes(exclude=['float64'])\n","    Xkat_dumm = pd.get_dummies(Xkat)\n","    X = pd.concat([Xfloat, Xkat_dumm], axis = 1)\n","#    dane_wyj = pd.concat([X, y], axis = 1)\n","    sc = StandardScaler()\n","    mmsc = MinMaxScaler()\n","    dane_wyj_float = dane_wyj.select_dtypes('float64')\n","#    dane_wyj_int = dane_wyj.select_dtypes('int64')\n","#    dane_wyj_obj = dane_wyj.select_dtypes('object')\n","\n","    data_num = pd.concat([dane_wyj_float, dane_wyj['fraud_wyk']], axis = 1)\n","    print(\"Postać danych data_num\")\n","    print(data_num.shape)\n","    selekcja_cech_num(data_num)\n","\n","    data_kat = dane_wyj.select_dtypes(exclude=['float64'])\n","    print(\"Postać danych data_kat\")\n","    print(data_kat.shape)\n","\n","\n","    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.33)\n","\n","    print('W funkcji przygotowanie_danych', type(Xtrain), Xtrain.shape, type(ytrain), ytrain.shape,\n","          type(Xtest), Xtest.shape, type(ytest), ytest.shape)\n","    Xtrain_float = Xtrain.select_dtypes('float64')\n","    Xtest_float = Xtest.select_dtypes('float64')\n","    Xtrain_reszta = Xtrain.select_dtypes(exclude=['float64'])\n","    Xtest_reszta = Xtest.select_dtypes(exclude=['float64'])\n","    mmsc.fit(Xtrain_float)\n","    Xtrain_float = mmsc.transform(Xtrain_float)\n","    mmsc.transform(Xtest_float)\n","    Xtrain = pd.concat([Xtrain_float, Xtrain_reszta], axis = 1)\n","    Xtest = pd.concat([Xtest_float, Xtest_reszta], axis = 1)\n","\n","    return Xtrain, Xtest, ytrain, ytest"],"metadata":{"id":"OfWVOmGtb8hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def przygotowanie_danych(df_prim):\n","    print(\"hihi\")\n","    dane_wyj = czyszczenie_danych(df_prim)\n","    print(\"Postać danych dane_wyj\")\n","    print(dane_wyj.shape)\n","    # Kodowanie danych typu 'int' oraz 'object'\n","    X, y = dane_wyj.drop(columns = 'fraud_wyk'), dane_wyj[['fraud_wyk']]\n","    print(type(X), X.shape, type(y), y.shape)\n","    Xfloat = X.select_dtypes('float64')\n","    Xkat = X.select_dtypes(exclude=['float64'])\n","    Xkat_dumm = pd.get_dummies(Xkat)\n","    X = pd.concat([Xfloat, Xkat_dumm], axis = 1)\n","#    dane_wyj = pd.concat([X, y], axis = 1)\n","\n","    mmsc = MinMaxScaler()\n","\n","    dane_wyj_float = dane_wyj.select_dtypes('float64')\n","\n","    data_num = pd.concat([dane_wyj_float, dane_wyj['fraud_wyk']], axis = 1)\n","    print(\"Postać danych data_num\")\n","    print(data_num.shape)\n","\n","    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.33)\n","\n","    print('W funkcji przygotowanie_danych', type(Xtrain), Xtrain.shape, type(ytrain), ytrain.shape,\n","          type(Xtest), Xtest.shape, type(ytest), ytest.shape)\n","    Xtrain_float = Xtrain.select_dtypes('float64')\n","    Xtest_float = Xtest.select_dtypes('float64')\n","    Xtrain_reszta = Xtrain.select_dtypes(exclude=['float64'])\n","    Xtest_reszta = Xtest.select_dtypes(exclude=['float64'])\n","\n","    mmsc.fit(Xtrain_float)\n","    Xtrain_float = mmsc.transform(Xtrain_float)  # Przypisanie danych przekształconych\n","    Xtest_float = mmsc.transform(Xtest_float)   # Przypisanie danych przekształconych\n","\n","    Xtrain = pd.concat([Xtrain_float, Xtrain_reszta], axis = 1)\n","    Xtest = pd.concat([Xtest_float, Xtest_reszta], axis = 1)\n","\n","    return Xtrain, Xtest, ytrain, ytest"],"metadata":{"id":"WUenFIe1E2ce"},"execution_count":null,"outputs":[]}]}