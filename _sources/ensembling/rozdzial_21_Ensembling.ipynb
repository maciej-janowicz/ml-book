{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Łączenie modeli i metody zespołowe w uczeniu maszynowym\n",
        "\n",
        "## Wprowadzenie\n",
        "\n",
        "Metody zespołowe (ang. \"ensembling\") to technika łączenia wielu modeli uczenia maszynowego w celu uzyskania lepszych wyników niż pojedynczy model. Idea opiera się na zasadzie \"mądrość tłumu\" - zespół przeciętnych modeli może przewyższyć jeden doskonały model.\n",
        "\n",
        "### Dlaczego ensembling ma szanse działać?\n",
        "\n",
        "- Redukcja wariancji - uśrednianie predykcji zmniejsza ryzyko przeuczenia\n",
        "-  Redukcja bias - różne modele mogą kompensować swoje słabości\n",
        "-  Lepsza generalizacja - zespół jest bardziej stabilny na nowych danych\n",
        "-  Odporność - mniejsza wrażliwość na szum i wartości odstające\n",
        "\n",
        "### Główne typy modeli zespołowych\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "- Idea: Trenuj wiele modeli na różnych podzbiorach danych (z powtórzeniami)\n",
        "- Agregacja: Głosowanie (klasyfikacja) lub uśrednianie (regresja)\n",
        "- Przykłady: Random Forest, Bagging Classifier\n",
        "- Zalety: Redukuje wariancję, działa dobrze z niestabilnymi modelami\n",
        "- Kiedy używać: Gdy model ma tendencję do przeuczenia\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "- Idea: Trenuj modele sekwencyjnie, każdy kolejny koncentruje się na błędach poprzedniego\n",
        "- Agregacja: Ważona suma predykcji\n",
        "- Przykłady: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "- Zalety: Redukuje bias i wariancję, często najlepsze wyniki\n",
        "- Kiedy używać: Gdy potrzebujesz najwyższej dokładności\n",
        "\n",
        "3. Stacking\n",
        "\n",
        "- Idea: Trenuj meta-model, który uczy się kombinować predykcje bazowych modeli\n",
        "- Agregacja: Uczony meta-model\n",
        "- Zalety: Może łączyć różnorodne modele\n",
        "- Kiedy używać: Gdy masz czas na eksperymentowanie i chcesz wycisnąć ostatnie procenty wydajności\n",
        "\n",
        "4. Voting/Blending\n",
        "\n",
        "- Idea: Prosta kombinacja predykcji (hard/soft voting)\n",
        "- Agregacja: Głosowanie lub uśrednianie\n",
        "- Zalety: Prosty, szybki, łatwy do zaimplementowania\n",
        "- Kiedy używać: Jako pierwszy krok w ensemblingu\n",
        "\n",
        "## Podstawowe zasady efektywnego ensemblingu\n",
        "\n",
        "- Różnorodność modeli - modele powinny popełniać różne błędy\n",
        "- Jakość bazowych modeli - modele nie mogą być zbyt słabe\n",
        "- Balans bias-variance - łącz modele o różnych charakterystykach\n",
        "- Walidacja krzyżowa - oceniaj zespół na oddzielnych danych"
      ],
      "metadata": {
        "id": "uvOPxxXIyHbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ZaTDGff5MZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "7ba79ec0-6dc1-4fae-a400-e07d9023f0c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-1076035410.py, line 359)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1076035410.py\"\u001b[0;36m, line \u001b[0;32m359\u001b[0m\n\u001b[0;31m    print(\"- Boosting redukuje zarówno obciążenie (\"bias\") jak i wariancję\")\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Metody ensemble\n",
        "from sklearn.ensemble import (\n",
        "    BaggingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    VotingClassifier,\n",
        "    StackingClassifier\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Generowanie danych\n",
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLING DLA KLASYFIKACJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "np.random.seed(42)\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=3,\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.3, 0.3, 0.4],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nRozmiar zbioru treningowego: {X_train.shape}\")\n",
        "print(f\"Rozmiar zbioru testowego: {X_test.shape}\")\n",
        "print(f\"Liczba klas: {len(np.unique(y))}\")\n",
        "print(f\"Rozkład klas w zbiorze treningowym: {np.bincount(y_train)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 1: POJEDYNCZE MODELE BAZOWE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 1: POJEDYNCZE MODELE BAZOWE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "base_models = {\n",
        "    'Drzewo decyzyjne': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    'Regresja logistyczna': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "base_results = {}\n",
        "\n",
        "for name, model in base_models.items():\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    base_results[name] = {\n",
        "        'mean': scores.mean(),\n",
        "        'std': scores.std()\n",
        "    }\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 2: BAGGING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 2: BAGGING - REDUKCJA WARIANCJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Bagging z Decision Tree\n",
        "bagging_dt = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    max_features=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_bagging = cross_val_score(bagging_dt, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nBagging (drzewo decyzyjne, n=50):\")\n",
        "print(f\"  CV Accuracy: {scores_bagging.mean():.4f} (+/- {scores_bagging.std():.4f})\")\n",
        "print(f\"  Poprawa vs pojedyncze drzewo: {scores_bagging.mean() - base_results['Drzewo decyzyjne']['mean']:.4f}\")\n",
        "\n",
        "# Random Forest (specjalizowana wersja baggingu)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nRandom Forest (n=100):\")\n",
        "print(f\"  CV Accuracy: {scores_rf.mean():.4f} (+/- {scores_rf.std():.4f})\")\n",
        "\n",
        "# Analiza wpływu liczby estimatorów\n",
        "print(\"\\nAnaliza wpływu liczby drzew w Random Forest:\")\n",
        "n_estimators_range = [10, 25, 50, 100, 200]\n",
        "rf_scores_by_n = []\n",
        "\n",
        "for n in n_estimators_range:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        max_depth=10,\n",
        "        random_state=42\n",
        "    )\n",
        "    scores = cross_val_score(rf_temp, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    rf_scores_by_n.append(scores.mean())\n",
        "    print(f\"  n={n:3d}: {scores.mean():.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, rf_scores_by_n, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Liczba drzew')\n",
        "plt.ylabel('CV Accuracy')\n",
        "plt.title('Wpływ liczby drzew na wydajność Random Forest')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 3: BOOSTING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 3: BOOSTING - REDUKCJA BIAS I WARIANCJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# AdaBoost\n",
        "adaboost = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=3),\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_ada = cross_val_score(adaboost, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nAdaBoost (n=100, lr=1.0):\")\n",
        "print(f\"  CV Accuracy: {scores_ada.mean():.4f} (+/- {scores_ada.std():.4f})\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_gb = cross_val_score(gb, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nGradient Boosting (n=100, lr=0.1):\")\n",
        "print(f\"  CV Accuracy: {scores_gb.mean():.4f} (+/- {scores_gb.std():.4f})\")\n",
        "\n",
        "# Analiza wpływu learning rate\n",
        "print(\"\\nAnaliza wpływu learning rate w Gradient Boosting:\")\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]\n",
        "gb_scores_by_lr = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    gb_temp = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=lr,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "    scores = cross_val_score(gb_temp, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    gb_scores_by_lr.append(scores.mean())\n",
        "    print(f\"  lr={lr:.2f}: {scores.mean():.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(learning_rates, gb_scores_by_lr, 'go-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('CV Accuracy')\n",
        "plt.title('Wpływ learning rate na wydajność Gradient Boosting')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 4: VOTING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 4: VOTING - PROSTA AGREGACJA MODELI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hard Voting - głosowanie większościowe\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
        "        ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "    ],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "scores_voting_hard = cross_val_score(voting_hard, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nHard Voting (RF + GB + SVM):\")\n",
        "print(f\"  CV Accuracy: {scores_voting_hard.mean():.4f} (+/- {scores_voting_hard.std():.4f})\")\n",
        "\n",
        "# Soft Voting - uśrednianie prawdopodobieństw\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
        "        ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "scores_voting_soft = cross_val_score(voting_soft, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nSoft Voting (RF + GB + SVM):\")\n",
        "print(f\"  CV Accuracy: {scores_voting_soft.mean():.4f} (+/- {scores_voting_soft.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 5: STACKING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 5: STACKING - META-MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Definicja modeli bazowych\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
        "    ('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# Meta-model - Logistic Regression\n",
        "stacking = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "scores_stacking = cross_val_score(stacking, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nStacking (RF + GB + SVM + KNN → RegLog):\")\n",
        "print(f\"  CV Accuracy: {scores_stacking.mean():.4f} (+/- {scores_stacking.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 6: PORÓWNANIE WSZYSTKICH METOD\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 6: PORÓWNANIE WSZYSTKICH METOD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Zbieranie wyników\n",
        "all_results = {\n",
        "    'Drzewo decyzyjne': base_results['Drzewo decyzyjne']['mean'],\n",
        "    'Random Forest': scores_rf.mean(),\n",
        "    'Bagging': scores_bagging.mean(),\n",
        "    'AdaBoost': scores_ada.mean(),\n",
        "    'Gradient Boosting': scores_gb.mean(),\n",
        "    'Hard Voting': scores_voting_hard.mean(),\n",
        "    'Soft Voting': scores_voting_soft.mean(),\n",
        "    'Stacking': scores_stacking.mean()\n",
        "}\n",
        "\n",
        "# Sortowanie wyników\n",
        "sorted_results = dict(sorted(all_results.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "print(\"\\nRanking metod (od najlepszej):\")\n",
        "for i, (method, score) in enumerate(sorted_results.items(), 1):\n",
        "    print(f\"{i}. {method:20s}: {score:.4f}\")\n",
        "\n",
        "# Wizualizacja porównania\n",
        "plt.figure(figsize=(12, 6))\n",
        "methods = list(sorted_results.keys())\n",
        "scores = list(sorted_results.values())\n",
        "colors = ['red' if 'Drzewo' in m else 'green' if any(x in m for x in ['Bagging', 'Random', 'AdaBoost', 'Gradient'])\n",
        "          else 'blue' for m in methods]\n",
        "\n",
        "plt.barh(methods, scores, color=colors, alpha=0.7)\n",
        "plt.xlabel('CV Accuracy')\n",
        "plt.title('Porównanie metod ensemblingu')\n",
        "plt.xlim([min(scores) - 0.02, max(scores) + 0.02])\n",
        "for i, (method, score) in enumerate(zip(methods, scores)):\n",
        "    plt.text(score + 0.002, i, f'{score:.4f}', va='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 7: OCENA NAJLEPSZEGO MODELU NA ZBIORZE TESTOWYM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 7: OCENA NA ZBIORZE TESTOWYM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Wybierz najlepszy model\n",
        "best_model_name = list(sorted_results.keys())[0]\n",
        "print(f\"\\nNajlepszy model: {best_model_name}\")\n",
        "\n",
        "# Trenuj i testuj najlepszy model (użyjmy Stackingu)\n",
        "stacking.fit(X_train, y_train)\n",
        "y_pred = stacking.predict(X_test)\n",
        "\n",
        "print(f\"\\nWyniki na zbiorze testowym:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"\\nRaport klasyfikacji:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "# Macierz pomyłek\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Macierz pomyłek - {best_model_name}')\n",
        "plt.ylabel('Prawdziwa klasa')\n",
        "plt.xlabel('Przewidziana klasa')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analiza błędów poszczególnych modeli bazowych\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALIZA RÓŻNORODNOŚCI MODELI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Trenuj modele bazowe\n",
        "base_predictions = {}\n",
        "for name, model in base_learners:\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    base_predictions[name] = pred\n",
        "    print(f\"\\n{name} - Dokładność: {accuracy_score(y_test, pred):.4f}\")\n",
        "\n",
        "# Analiza zgodności predykcji\n",
        "print(\"\\nMacierz zgodności predykcji między modelami:\")\n",
        "agreement_matrix = np.zeros((len(base_learners), len(base_learners)))\n",
        "\n",
        "for i, (name1, pred1) in enumerate(base_predictions.items()):\n",
        "    for j, (name2, pred2) in enumerate(base_predictions.items()):\n",
        "        agreement = np.mean(pred1 == pred2)\n",
        "        agreement_matrix[i, j] = agreement\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(agreement_matrix, annot=True, fmt='.3f',\n",
        "            xticklabels=base_predictions.keys(),\n",
        "            yticklabels=base_predictions.keys(),\n",
        "            cmap='YlOrRd', vmin=0, vmax=1)\n",
        "plt.title('Zgodność predykcji między modelami\\n(1.0 = całkowita zgodność)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nWnioski:\")\n",
        "print(\"- Metody zespołowe często poprawiają wyniki, uzyskane za pomocą pojedynczych modeli\")\n",
        "print(\"- Bagging redukuje wariancję (działa najlepiej z niestabilnymi modelami)\")\n",
        "print(\"- Boosting redukuje zarówno obciążenie (\"bias\") jak i wariancję\")\n",
        "print(\"- Stacking może osiągnąć najlepsze wyniki, ale jest najbardziej złożony\")\n",
        "print(\"- Różnorodność modeli bazowych jest istotna dla metod zespołowych\")\n",
        "print(\"- Im mniejsza zgodność między modelami, tym większy potencjał ensemblingu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przykład 2: Regresja - Ensemble dla przewidywania wartości ciągłych"
      ],
      "metadata": {
        "id": "RAVJ0fGZ1yLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Ensemble methods\n",
        "from sklearn.ensemble import (\n",
        "    BaggingRegressor,\n",
        "    RandomForestRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    VotingRegressor,\n",
        "    StackingRegressor\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLING DLA REGRESJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Załadowanie danych\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nProblem: Przewidywanie cen domów w Kalifornii\")\n",
        "print(f\"Rozmiar zbioru treningowego: {X_train.shape}\")\n",
        "print(f\"Rozmiar zbioru testowego: {X_test.shape}\")\n",
        "print(f\"Zakres wartości docelowej: [{y.min():.2f}, {y.max():.2f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 1: MODELE BAZOWE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 1: MODELE BAZOWE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "base_models = {\n",
        "    'Drzewo decyzyjne': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "    'Regresja liniowa': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'SVR': SVR(kernel='rbf'),\n",
        "    'KNN': KNeighborsRegressor(n_neighbors=5)\n",
        "}\n",
        "\n",
        "base_results = {}\n",
        "\n",
        "for name, model in base_models.items():\n",
        "    # Cross-validation RMSE\n",
        "    scores = -cross_val_score(model, X_train, y_train, cv=5,\n",
        "                              scoring='neg_root_mean_squared_error')\n",
        "    base_results[name] = {\n",
        "        'mean': scores.mean(),\n",
        "        'std': scores.std()\n",
        "    }\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  CV RMSE: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 2: BAGGING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 2: BAGGING - STABILIZACJA PREDYKCJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Bagging z Decision Tree\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    max_features=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_bagging = -cross_val_score(bagging, X_train, y_train, cv=5,\n",
        "                                  scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nBagging (drzewo decyzyjne, n=50):\")\n",
        "print(f\"  CV RMSE: {scores_bagging.mean():.4f} (+/- {scores_bagging.std():.4f})\")\n",
        "print(f\"  Redukcja RMSE vs drzewo: {base_results['Drzewo decyzyjne']['mean'] - scores_bagging.mean():.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_rf = -cross_val_score(rf, X_train, y_train, cv=5,\n",
        "                             scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nRandom Forest (n=100):\")\n",
        "print(f\"  CV RMSE: {scores_rf.mean():.4f} (+/- {scores_rf.std():.4f})\")\n",
        "\n",
        "# Analiza feature importance\n",
        "rf.fit(X_train, y_train)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'cecha': data.feature_names,\n",
        "    'waznosc': rf.feature_importances_\n",
        "}).sort_values('waznosc', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 5 najważniejszych cech (Random Forest):\")\n",
        "print(feature_importance.head())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['cecha'], feature_importance['waznosc'])\n",
        "plt.xlabel('Ważność cechy')\n",
        "plt.title('Ważność cech w Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 3: BOOSTING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 3: BOOSTING - SEKWENCYJNE UCZENIE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# AdaBoost\n",
        "adaboost = AdaBoostRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=5),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_ada = -cross_val_score(adaboost, X_train, y_train, cv=5,\n",
        "                              scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nAdaBoost (n=100, lr=0.5):\")\n",
        "print(f\"  CV RMSE: {scores_ada.mean():.4f} (+/- {scores_ada.std():.4f})\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_gb = -cross_val_score(gb, X_train, y_train, cv=5,\n",
        "                             scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nGradient Boosting (n=100, lr=0.1):\")\n",
        "print(f\"  CV RMSE: {scores_gb.mean():.4f} (+/- {scores_gb.std():.4f})\")\n",
        "\n",
        "# Analiza wpływu liczby iteracji na Gradient Boosting\n",
        "print(\"\\nAnaliza błędu treningowego vs walidacyjnego (Gradient Boosting):\")\n",
        "\n",
        "gb_analyze = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_analyze.fit(X_train, y_train)\n",
        "\n",
        "# Błędy w funkcji liczby iteracji\n",
        "train_scores = []\n",
        "val_scores = []\n",
        "\n",
        "for i, (train_pred, val_pred) in enumerate(zip(\n",
        "    gb_analyze.staged_predict(X_train),\n",
        "    gb_analyze.staged_predict(X_test)\n",
        ")):\n",
        "    train_scores.append(mean_squared_error(y_train, train_pred))\n",
        "    val_scores.append(mean_squared_error(y_test, val_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_scores) + 1), train_scores,\n",
        "         label='Błąd treningowy', linewidth=2)\n",
        "plt.plot(range(1, len(val_scores) + 1), val_scores,\n",
        "         label='Błąd walidacyjny', linewidth=2)\n",
        "plt.xlabel('Liczba iteracji')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Krzywa uczenia Gradient Boosting')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_iterations = np.argmin(val_scores) + 1\n",
        "print(f\"Optymalna liczba iteracji: {optimal_iterations}\")\n",
        "print(f\"MSE przy optymalnej liczbie iteracji: {val_scores[optimal_iterations-1]:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 4: VOTING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 4: VOTING - UŚREDNIANIE PREDYKCJI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Voting Regressor - średnia z różnych modeli\n",
        "voting = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),\n",
        "        ('ridge', Ridge(alpha=1.0))\n",
        "    ]\n",
        ")\n",
        "\n",
        "scores_voting = -cross_val_score(voting, X_train, y_train, cv=5,\n",
        "                                 scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nVoting (RF + GB + Ridge):\")\n",
        "print(f\"  CV RMSE: {scores_voting.mean():.4f} (+/- {scores_voting.std():.4f})\")\n",
        "\n",
        "# Voting z wagami\n",
        "voting_weighted = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),\n",
        "        ('ridge', Ridge(alpha=1.0))\n",
        "    ],\n",
        "    weights=[2, 3, 1]  # Większa waga dla Gradient Boosting\n",
        ")\n",
        "\n",
        "scores_voting_weighted = -cross_val_score(voting_weighted, X_train, y_train, cv=5,\n",
        "                                         scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nVoting Weighted (wagi: RF=2, GB=3, Ridge=1):\")\n",
        "print(f\"  CV RMSE: {scores_voting_weighted.mean():.4f} (+/- {scores_voting_weighted.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 5: STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 5: STACKING - META-REGRESSOR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Modele bazowe\n",
        "base_learners = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
        "    ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42)),\n",
        "    ('ridge', Ridge(alpha=1.0)),\n",
        "    ('svr', SVR(kernel='rbf', C=1.0))\n",
        "]\n",
        "\n",
        "# Meta-model\n",
        "stacking = StackingRegressor(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=Ridge(alpha=0.5),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "scores_stacking = -cross_val_score(stacking, X_train, y_train, cv=5,\n",
        "                                   scoring='neg_root_mean_squared_error')\n",
        "print(f\"\\nStacking (RF + GB + Ridge + SVR → Ridge):\")\n",
        "print(f\"  CV RMSE: {scores_stacking.mean():.4f} (+/- {scores_stacking.std():.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 6: PORÓWNANIE I WIZUALIZACJA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 6: PORÓWNANIE WSZYSTKICH METOD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = {\n",
        "    'Drzewo decyzyjne': base_results['Drzewo decyzyjne']['mean'],\n",
        "    'Regresja liniowa': base_results['Regresja liniowa']['mean'],\n",
        "    'Ridge': base_results['Ridge']['mean'],\n",
        "    'Bagging': scores_bagging.mean(),\n",
        "    'Random Forest': scores_rf.mean(),\n",
        "    'AdaBoost': scores_ada.mean(),\n",
        "    'Gradient Boosting': scores_gb.mean(),\n",
        "    'Voting': scores_voting.mean(),\n",
        "    'Voting Weighted': scores_voting_weighted.mean(),\n",
        "    'Stacking': scores_stacking.mean()\n",
        "}\n",
        "\n",
        "sorted_results = dict(sorted(all_results.items(), key=lambda x: x[1]))\n",
        "\n",
        "print(\"\\nRanking metod (od najlepszej - najniższy RMSE):\")\n",
        "for i, (method, rmse) in enumerate(sorted_results.items(), 1):\n",
        "    print(f\"{i:2d}. {method:20s}: RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Wizualizacja\n",
        "plt.figure(figsize=(12, 8))\n",
        "methods = list(sorted_results.keys())\n",
        "rmses = list(sorted_results.values())\n",
        "colors = ['red' if any(x in m for x in ['Drzewo', 'liniowa', 'Ridge'])\n",
        "          else 'green' if any(x in m for x in ['Bagging', 'Random', 'AdaBoost', 'Gradient'])\n",
        "          else 'blue' for m in methods]\n",
        "\n",
        "plt.barh(methods, rmses, color=colors, alpha=0.7)\n",
        "plt.xlabel('RMSE (niższy = lepszy)')\n",
        "plt.title('Porównanie metod ensemblingu dla regresji')\n",
        "for i, (method, rmse) in enumerate(zip(methods, rmses)):\n",
        "    plt.text(rmse + 0.01, i, f'{rmse:.4f}', va='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 7: SZCZEGÓŁOWA OCENA NAJLEPSZEGO MODELU\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 7: SZCZEGÓŁOWA OCENA NAJLEPSZEGO MODELU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Trenuj najlepszy model\n",
        "best_model = stacking\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predykcje\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "print(f\"\\nNajlepszy model: Stacking Regressor\")\n",
        "print(f\"\\nWyniki na zbiorze treningowym:\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
        "print(f\"  R²: {r2_score(y_train, y_train_pred):.4f}\")\n",
        "\n",
        "print(f\"\\nWyniki na zbiorze testowym:\")\n",
        "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
        "print(f\"  MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
        "print(f\"  R²: {r2_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "# Wizualizacja predykcji\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Scatter plot\n",
        "axes[0].scatter(y_test, y_test_pred, alpha=0.5, s=30)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "             'r--', lw=2, label='Idealna predykcja')\n",
        "axes[0].set_xlabel('Rzeczywiste wartości')\n",
        "axes[0].set_ylabel('Przewidywane wartości')\n",
        "axes[0].set_title('Predykcje vs Rzeczywiste wartości')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test - y_test_pred\n",
        "axes[1].scatter(y_test_pred, residuals, alpha=0.5, s=30)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Przewidywane wartości')\n",
        "axes[1].set_ylabel('Residuals (błędy)')\n",
        "axes[1].set_title('Wykres residuów')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analiza błędów modeli bazowych vs ensemble\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALIZA KOMPLEMENTARNOŚCI MODELI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Predykcje modeli bazowych\n",
        "base_predictions = {}\n",
        "for name, model in base_learners:\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "    base_predictions[name] = pred\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\")\n",
        "    print(f\"  Korelacja z residuami innych modeli:\")\n",
        "\n",
        "    for name2, pred2 in base_predictions.items():\n",
        "        if name != name2:\n",
        "            residuals2 = y_test - pred2\n",
        "            correlation = np.corrcoef(pred, residuals2)[0, 1]\n",
        "            print(f\"    vs {name2}: {correlation:.4f}\")\n",
        "\n",
        "print(\"\\nWnioski:\")\n",
        "print(\"- Metody zespołowe redukują błąd predykcji w porównaniu do pojedynczych modeli\")\n",
        "print(\"- Gradient Boosting często osiąga najlepsze wyniki dla regresji\")\n",
        "print(\"- Stacking może dalej poprawić wyniki łącząc różnorodne modele\")\n",
        "print(\"- Voting jest prostszy w implementacji i daje dobre rezultaty\")\n",
        "print(\"- Modele bazowe powinny mieć niską korelację błędów dla efektywnego ensemblingu\")"
      ],
      "metadata": {
        "id": "uwiTMqx019Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przykład 3: Klasteryzacja metodami zespołowymi"
      ],
      "metadata": {
        "id": "SWy_n3T_2Mev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.stats import mode\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLE CLUSTERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generowanie danych\n",
        "np.random.seed(42)\n",
        "X, y_true = make_blobs(\n",
        "    n_samples=600,\n",
        "    n_features=2,\n",
        "    centers=4,\n",
        "    cluster_std=1.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Dodaj trochę szumu\n",
        "noise = np.random.normal(0, 0.5, X.shape)\n",
        "X = X + noise\n",
        "\n",
        "# Skalowanie\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"\\nRozmiar danych: {X.shape}\")\n",
        "print(f\"Liczba prawdziwych klastrów: {len(np.unique(y_true))}\")\n",
        "\n",
        "# Wizualizacja danych\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
        "plt.title('Dane wejściowe (prawdziwe klastry)')\n",
        "plt.xlabel('Cecha 1')\n",
        "plt.ylabel('Cecha 2')\n",
        "plt.colorbar(label='Klaster')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 1: POJEDYNCZE ALGORYTMY KLASTERYZACJI\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 1: POJEDYNCZE ALGORYTMY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_clusters = 4\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "labels_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Hierarchical Clustering (różne linkaże)\n",
        "agg_ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "labels_ward = agg_ward.fit_predict(X_scaled)\n",
        "\n",
        "agg_average = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
        "labels_average = agg_average.fit_predict(X_scaled)\n",
        "\n",
        "agg_complete = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\n",
        "labels_complete = agg_complete.fit_predict(X_scaled)\n",
        "\n",
        "# DBSCAN (wymaga innych parametrów)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Ocena pojedynczych algorytmów\n",
        "algorithms = {\n",
        "    'KMeans': labels_kmeans,\n",
        "    'Hierarchiczny (Ward)': labels_ward,\n",
        "    'Hierarchiczny (Average)': labels_average,\n",
        "    'Hierarchiczny (Complete)': labels_complete,\n",
        "    'DBSCAN': labels_dbscan\n",
        "}\n",
        "\n",
        "print(\"\\nOcena pojedynczych algorytmów:\")\n",
        "for name, labels in algorithms.items():\n",
        "    if len(np.unique(labels)) > 1:  # DBSCAN może znaleźć 1 klaster\n",
        "        silhouette = silhouette_score(X_scaled, labels)\n",
        "        davies_bouldin = davies_bouldin_score(X_scaled, labels)\n",
        "        ari = adjusted_rand_score(y_true, labels)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Liczba klastrów: {len(np.unique(labels))}\")\n",
        "        print(f\"  Silhouette: {silhouette:.4f}\")\n",
        "        print(f\"  Davies-Bouldin: {davies_bouldin:.4f}\")\n",
        "        print(f\"  ARI (vs prawdziwe): {ari:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n{name}: Znaleziono tylko 1 klaster\")\n",
        "\n",
        "# Wizualizacja wyników pojedynczych algorytmów\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Prawdziwe klastry\n",
        "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
        "axes[0].set_title('Prawdziwe klastry')\n",
        "\n",
        "# Algorytmy\n",
        "for i, (name, labels) in enumerate(algorithms.items(), 1):\n",
        "    axes[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
        "    axes[i].set_title(name)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 2: CONSENSUS CLUSTERING (Voting)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 2: CONSENSUS CLUSTERING - GŁOSOWANIE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nMetoda: Consensus clustering poprzez co-association matrix\")\n",
        "print(\"Idea: Zliczamy, jak często pary punktów są w tym samym klastrze\")\n",
        "\n",
        "# Użyjemy tylko algorytmów, które znalazły więcej niż 1 klaster\n",
        "valid_algorithms = {k: v for k, v in algorithms.items() if len(np.unique(v)) > 1}\n",
        "\n",
        "# Tworzenie macierzy współwystępowania\n",
        "n_samples = X_scaled.shape[0]\n",
        "n_algorithms = len(valid_algorithms)\n",
        "co_association_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "for name, labels in valid_algorithms.items():\n",
        "    for i in range(n_samples):\n",
        "        for j in range(i+1, n_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                co_association_matrix[i, j] += 1\n",
        "                co_association_matrix[j, i] += 1\n",
        "\n",
        "# Normalizacja\n",
        "co_association_matrix /= n_algorithms\n",
        "\n",
        "print(f\"\\nMacierz współwystępowania obliczona z {n_algorithms} algorytmów\")\n",
        "print(f\"Średnia wartość w macierzy: {co_association_matrix.mean():.4f}\")\n",
        "\n",
        "# Konwersja macierzy współwystępowania na macierz odległości\n",
        "distance_matrix = 1 - co_association_matrix\n",
        "\n",
        "# Hierarchical clustering na macierzy odległości\n",
        "consensus_clustering = AgglomerativeClustering(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='precomputed',\n",
        "    linkage='average'\n",
        ")\n",
        "labels_consensus = consensus_clustering.fit_predict(distance_matrix)\n",
        "\n",
        "# Ocena\n",
        "silhouette_consensus = silhouette_score(X_scaled, labels_consensus)\n",
        "davies_bouldin_consensus = davies_bouldin_score(X_scaled, labels_consensus)\n",
        "ari_consensus = adjusted_rand_score(y_true, labels_consensus)\n",
        "\n",
        "print(f\"\\nConsensus Clustering:\")\n",
        "print(f\"  Silhouette: {silhouette_consensus:.4f}\")\n",
        "print(f\"  Davies-Bouldin: {davies_bouldin_consensus:.4f}\")\n",
        "print(f\"  ARI (vs prawdziwe): {ari_consensus:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 3: ENSEMBLE Z RÓŻNYMI INICJALIZACJAMI\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 3: ENSEMBLE Z RÓŻNYMI INICJALIZACJAMI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nMetoda: Uruchom KMeans wielokrotnie z różnymi inicjalizacjami\")\n",
        "\n",
        "n_runs = 20\n",
        "kmeans_ensemble_labels = []\n",
        "\n",
        "for i in range(n_runs):\n",
        "    km = KMeans(n_clusters=n_clusters, random_state=i, n_init=1)\n",
        "    labels = km.fit_predict(X_scaled)\n",
        "    kmeans_ensemble_labels.append(labels)\n",
        "\n",
        "# Macierz współwystępowania dla ensemble KMeans\n",
        "co_assoc_kmeans = np.zeros((n_samples, n_samples))\n",
        "\n",
        "for labels in kmeans_ensemble_labels:\n",
        "    for i in range(n_samples):\n",
        "        for j in range(i+1, n_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                co_assoc_kmeans[i, j] += 1\n",
        "                co_assoc_kmeans[j, i] += 1\n",
        "\n",
        "co_assoc_kmeans /= n_runs\n",
        "\n",
        "# Consensus clustering\n",
        "distance_kmeans = 1 - co_assoc_kmeans\n",
        "ensemble_kmeans = AgglomerativeClustering(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='precomputed',\n",
        "    linkage='average'\n",
        ")\n",
        "labels_ensemble_kmeans = ensemble_kmeans.fit_predict(distance_kmeans)\n",
        "\n",
        "# Ocena\n",
        "silhouette_ens_km = silhouette_score(X_scaled, labels_ensemble_kmeans)\n",
        "davies_bouldin_ens_km = davies_bouldin_score(X_scaled, labels_ensemble_kmeans)\n",
        "ari_ens_km = adjusted_rand_score(y_true, labels_ensemble_kmeans)\n",
        "\n",
        "print(f\"\\nEnsemble KMeans ({n_runs} uruchomień):\")\n",
        "print(f\"  Silhouette: {silhouette_ens_km:.4f}\")\n",
        "print(f\"  Davies-Bouldin: {davies_bouldin_ens_km:.4f}\")\n",
        "print(f\"  ARI (vs prawdziwe): {ari_ens_km:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 4: ENSEMBLE Z RÓŻNYMI PODZBIORAMI CECH\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 4: ENSEMBLE Z RÓŻNYMI PODZBIORAMI CECH\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nMetoda: Klasteryzacja na losowych podzbiorach cech (Feature Bagging)\")\n",
        "print(\"Symulacja: Dodamy dodatkowe cechy szumu\")\n",
        "\n",
        "# Dodaj więcej wymiarów (cechy szumu)\n",
        "np.random.seed(42)\n",
        "X_extended = np.hstack([X_scaled, np.random.randn(n_samples, 8)])\n",
        "\n",
        "print(f\"Rozszerzony zbiór danych: {X_extended.shape[1]} cech\")\n",
        "\n",
        "n_feature_subsets = 15\n",
        "feature_subset_size = 5\n",
        "feature_ensemble_labels = []\n",
        "\n",
        "for i in range(n_feature_subsets):\n",
        "    # Losowy podzbiór cech\n",
        "    np.random.seed(i)\n",
        "    feature_indices = np.random.choice(X_extended.shape[1],\n",
        "                                      feature_subset_size,\n",
        "                                      replace=False)\n",
        "    X_subset = X_extended[:, feature_indices]\n",
        "\n",
        "    # KMeans na podzbiorze\n",
        "    km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = km.fit_predict(X_subset)\n",
        "    feature_ensemble_labels.append(labels)\n",
        "\n",
        "# Consensus clustering\n",
        "co_assoc_features = np.zeros((n_samples, n_samples))\n",
        "\n",
        "for labels in feature_ensemble_labels:\n",
        "    for i in range(n_samples):\n",
        "        for j in range(i+1, n_samples):\n",
        "            if labels[i] == labels[j]:\n",
        "                co_assoc_features[i, j] += 1\n",
        "                co_assoc_features[j, i] += 1\n",
        "\n",
        "co_assoc_features /= n_feature_subsets\n",
        "\n",
        "distance_features = 1 - co_assoc_features\n",
        "ensemble_features = AgglomerativeClustering(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='precomputed',\n",
        "    linkage='average'\n",
        ")\n",
        "labels_ensemble_features = ensemble_features.fit_predict(distance_features)\n",
        "\n",
        "# Ocena\n",
        "silhouette_ens_feat = silhouette_score(X_scaled, labels_ensemble_features)\n",
        "davies_bouldin_ens_feat = davies_bouldin_score(X_scaled, labels_ensemble_features)\n",
        "ari_ens_feat = adjusted_rand_score(y_true, labels_ensemble_features)\n",
        "\n",
        "print(f\"\\nFeature Bagging Ensemble ({n_feature_subsets} podzbiorów, {feature_subset_size} cech każdy):\")\n",
        "print(f\"  Silhouette: {silhouette_ens_feat:.4f}\")\n",
        "print(f\"  Davies-Bouldin: {davies_bouldin_ens_feat:.4f}\")\n",
        "print(f\"  ARI (vs prawdziwe): {ari_ens_feat:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SEKCJA 5: PORÓWNANIE WSZYSTKICH METOD\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SEKCJA 5: PORÓWNANIE WSZYSTKICH METOD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = {\n",
        "    'KMeans (pojedynczy)': {\n",
        "        'silhouette': silhouette_score(X_scaled, labels_kmeans),\n",
        "        'ari': adjusted_rand_score(y_true, labels_kmeans),\n",
        "        'labels': labels_kmeans\n",
        "    },\n",
        "    'Hierarchiczny Ward': {\n",
        "        'silhouette': silhouette_score(X_scaled, labels_ward),\n",
        "        'ari': adjusted_rand_score(y_true, labels_ward),\n",
        "        'labels': labels_ward\n",
        "    },\n",
        "    'Consensus (algorytmy)': {\n",
        "        'silhouette': silhouette_consensus,\n",
        "        'ari': ari_consensus,\n",
        "        'labels': labels_consensus\n",
        "    },\n",
        "    'Ensemble KMeans': {\n",
        "        'silhouette': silhouette_ens_km,\n",
        "        'ari': ari_ens_km,\n",
        "        'labels': labels_ensemble_kmeans\n",
        "    },\n",
        "    'Feature Bagging': {\n",
        "        'silhouette': silhouette_ens_feat,\n",
        "        'ari': ari_ens_feat,\n",
        "        'labels': labels_ensemble_features\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nRanking według Silhouette Score:\")\n",
        "sorted_by_silhouette = sorted(all_results.items(),\n",
        "                             key=lambda x: x[1]['silhouette'],\n",
        "                             reverse=True)\n",
        "for i, (method, scores) in enumerate(sorted_by_silhouette, 1):\n",
        "    print(f\"{i}. {method:30s}: Silhouette={scores['silhouette']:.4f}, ARI={scores['ari']:.4f}\")\n",
        "\n",
        "# Wizualizacja porównawcza\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (method, result) in enumerate(all_results.items()):\n",
        "    axes[i].scatter(X[:, 0], X[:, 1], c=result['labels'],\n",
        "                   cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
        "    axes[i].set_title(f\"{method}\\nSilhouette: {result['silhouette']:.4f}, ARI: {result['ari']:.4f}\")\n",
        "    axes[i].set_xlabel('Cecha 1')\n",
        "    axes[i].set_ylabel('Cecha 2')\n",
        "\n",
        "# Prawdziwe klastry dla porównania\n",
        "axes[5].scatter(X[:, 0], X[:, 1], c=y_true,\n",
        "               cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
        "axes[5].set_title('Prawdziwe klastry')\n",
        "axes[5].set_xlabel('Cecha 1')\n",
        "axes[5].set_ylabel('Cecha 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Porównanie metryk\n",
        "methods = list(all_results.keys())\n",
        "silhouettes = [all_results[m]['silhouette'] for m in methods]\n",
        "aris = [all_results[m]['ari'] for m in methods]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].barh(methods, silhouettes, color='steelblue', alpha=0.7)\n",
        "axes[0].set_xlabel('Silhouette Score')\n",
        "axes[0].set_title('Silhouette Score (wyższy = lepszy)')\n",
        "for i, v in enumerate(silhouettes):\n",
        "    axes[0].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
        "\n",
        "axes[1].barh(methods, aris, color='coral', alpha=0.7)\n",
        "axes[1].set_xlabel('Adjusted Rand Index')\n",
        "axes[1].set_title('ARI vs prawdziwe klastry (wyższy = lepszy)')\n",
        "for i, v in enumerate(aris):\n",
        "    axes[1].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Wizualizacja macierzy współwystępowania\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Consensus algorytmów\n",
        "im1 = axes[0].imshow(co_association_matrix, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "axes[0].set_title('Co-Association Matrix\\n(Consensus algorytmów)')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# Ensemble KMeans\n",
        "im2 = axes[1].imshow(co_assoc_kmeans, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "axes[1].set_title('Co-Association Matrix\\n(Ensemble KMeans)')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "# Feature Bagging\n",
        "im3 = axes[2].imshow(co_assoc_features, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "axes[2].set_title('Co-Association Matrix\\n(Feature Bagging)')\n",
        "plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nWnioski:\")\n",
        "print(\"- Klasteryzacja zespołowa na ogół zwiększa stabilność i jakość klasteryzacji\")\n",
        "print(\"- 'Consensus clustering' łączy mocne strony różnych algorytmów\")\n",
        "print(\"- Wielokrotne uruchomienia z różnymi inicjalizacjami redukują wpływ losowości\")\n",
        "print(\"- 'Feature bagging' pomaga, gdy mamy wiele cech, z których część może być szumem\")\n",
        "print(\"- 'Co-association matrix' wizualizuje pewność przypisania do klastrów\")\n",
        "print(\"- Metody zespołowe są szczególnie przydatne gdy struktura klastrów jest niejednoznaczna\")"
      ],
      "metadata": {
        "id": "nr4FARHH2QRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}