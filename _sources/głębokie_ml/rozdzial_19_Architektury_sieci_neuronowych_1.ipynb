{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Podstawy i ewolucja architektur neuronowych. Część 1\n",
        "\n",
        "## Spis treści\n",
        "\n",
        "1. [Wprowadzenie](#wprowadzenie)\n",
        "2. [Historia rozwoju sieci neuronowych](#11-historia-rozwoju-sieci-neuronowych)\n",
        "3. [Kluczowe problemy klasycznych architektur](#12-kluczowe-problemy-klasycznych-architektur)\n",
        "4. [Przełomowe innowacje techniczne](#13-przełomowe-innowacje-techniczne)\n",
        "5. [Dlaczego potrzebujemy nowych architektur](#14-dlaczego-potrzebujemy-nowych-architektur)\n",
        "6. [Podsumowanie modułu](#podsumowanie-modułu-1)\n",
        "\n",
        "---\n",
        "\n",
        "## Wprowadzenie\n",
        "\n",
        "Aby zrozumieć nowoczesne architektury sieci neuronowych, musimy najpierw poznać historię ich rozwoju i kluczowe problemy, które doprowadziły do powstania innowacyjnych rozwiązań. Ten moduł przedstawia fundament, na którym zbudowane są wszystkie współczesne architektury.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Historia rozwoju sieci neuronowych\n",
        "\n",
        "### Perceptron (1958) - Początek ery\n",
        "\n",
        "Frank Rosenblatt stworzył **perceptron** - pierwszą implementację sztucznego neuronu zdolnego do uczenia się. Perceptron to prosty model liniowy, który może klasyfikować dane liniowo separowalne.\n",
        "\n",
        "**Kluczowa idea:** Neuron sumuje ważone wejścia i aktywuje się, gdy suma przekracza próg.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prosty perceptron dla problemu AND\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.1, n_iter=100):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.weights = np.random.randn(X.shape[1]) * 0.01\n",
        "        self.bias = 0\n",
        "        \n",
        "        for _ in range(self.n_iter):\n",
        "            for xi, yi in zip(X, y):\n",
        "                pred = 1 if np.dot(xi, self.weights) + self.bias >= 0 else 0\n",
        "                update = self.lr * (yi - pred)\n",
        "                self.weights += update * xi\n",
        "                self.bias += update\n",
        "\n",
        "# Problem AND\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "p = Perceptron()\n",
        "p.fit(X, y)\n",
        "print(\"Perceptron nauczył się AND!\")\n",
        "```\n",
        "\n",
        "**Ograniczenie:** Nie może nauczyć się funkcji XOR (nie jest liniowo separowalna).\n",
        "\n",
        "---\n",
        "\n",
        "### Zima AI i przełom lat 80.\n",
        "\n",
        "W latach 1969-1986 rozwój sieci neuronowych prawie zamarł z powodu niemożności rozwiązania problemu XOR i braku algorytmów trenowania wielowarstwowych sieci.\n",
        "\n",
        "**Przełom:** Odkrycie **wstecznej propagacji błędu** (backpropagation) przez Rumelharta, Hintona i Williamsa w 1986 roku umożliwiło trenowanie wielowarstwowych sieci.\n",
        "\n",
        "---\n",
        "\n",
        "### Wsteczna propagacja błędu\n",
        "\n",
        "Backpropagation to algorytm, który efektywnie oblicza gradienty funkcji straty względem wag sieci, wykorzystując **regułę łańcuchową**.\n",
        "\n",
        "```python\n",
        "# Prosta 2-warstwowa sieć z backpropagation\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.hidden = self.sigmoid(np.dot(X, self.W1))\n",
        "        self.output = self.sigmoid(np.dot(self.hidden, self.W2))\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, X, y, lr=0.1):\n",
        "        # Gradient na wyjściu\n",
        "        output_error = self.output - y\n",
        "        output_delta = output_error * self.output * (1 - self.output)\n",
        "        \n",
        "        # Propagacja do warstwy ukrytej (reguła łańcuchowa!)\n",
        "        hidden_error = output_delta.dot(self.W2.T)\n",
        "        hidden_delta = hidden_error * self.hidden * (1 - self.hidden)\n",
        "        \n",
        "        # Aktualizacja wag\n",
        "        self.W2 -= lr * self.hidden.T.dot(output_delta)\n",
        "        self.W1 -= lr * X.T.dot(hidden_delta)\n",
        "\n",
        "# XOR - niemożliwe dla perceptronu, ale możliwe dla sieci wielowarstwowej!\n",
        "X_xor = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "nn = NeuralNetwork(2, 4, 1)\n",
        "for _ in range(5000):\n",
        "    nn.forward(X_xor)\n",
        "    nn.backward(X_xor, y_xor)\n",
        "\n",
        "print(\"Sieć nauczyła się XOR!\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Głębokie sieci neuronowe (2006-2012)\n",
        "\n",
        "Renesans sieci neuronowych dzięki:\n",
        "- **Pre-training** (Hinton, 2006)\n",
        "- **ReLU activation** (szybsza niż sigmoid)\n",
        "- **Dropout** (regularyzacja)\n",
        "- **GPU computing** (tysiące razy szybsze trenowanie)\n",
        "\n",
        "---\n",
        "\n",
        "### Era ImageNet i przełom 2012 roku\n",
        "\n",
        "**AlexNet (2012)** - przełom w deep learning:\n",
        "- Pierwsza głęboka CNN, która wygrała ILSVRC\n",
        "- Błąd: 16.4% (vs 26% tradycyjne metody)\n",
        "- Wykorzystanie GPU, ReLU, dropout\n",
        "\n",
        "```python\n",
        "# Wizualizacja postępu ImageNet\n",
        "years = [2010, 2011, 2012, 2013, 2014, 2015, 2017]\n",
        "errors = [28.2, 25.8, 16.4, 11.7, 7.3, 3.6, 2.25]\n",
        "models = ['Traditional', 'Traditional', 'AlexNet', 'ZFNet',\n",
        "          'GoogLeNet', 'ResNet', 'SENet']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(years, errors, 'o-', linewidth=3, markersize=10)\n",
        "plt.axhline(y=5.1, color='r', linestyle='--', label='Human-level')\n",
        "plt.xlabel('Rok')\n",
        "plt.ylabel('Top-5 Error (%)')\n",
        "plt.title('Rewolucja Deep Learning (ImageNet)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Kluczowe problemy klasycznych architektur\n",
        "\n",
        "### Problem zanikającego/eksplodującego gradientu\n",
        "\n",
        "**Zanikający gradient:** W głębokich sieciach gradienty stają się coraz mniejsze, co powoduje, że wczesne warstwy uczą się bardzo wolno.\n",
        "\n",
        "**Eksplodujący gradient:** Gradienty rosną wykładniczo, powodując niestabilność.\n",
        "\n",
        "```python\n",
        "# Symulacja zanikającego gradientu\n",
        "def gradient_flow(n_layers, activation='sigmoid'):\n",
        "    gradient = 1.0\n",
        "    gradients = [gradient]\n",
        "    \n",
        "    for _ in range(n_layers):\n",
        "        weight = np.random.randn() * 0.5\n",
        "        # Sigmoid ma pochodną max 0.25\n",
        "        activation_deriv = np.random.uniform(0.1, 0.25)\n",
        "        gradient *= weight * activation_deriv\n",
        "        gradients.append(abs(gradient))\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "# Porównanie sigmoid vs ReLU\n",
        "grads_sigmoid = gradient_flow(30, 'sigmoid')\n",
        "plt.semilogy(grads_sigmoid, label='Sigmoid')\n",
        "plt.xlabel('Warstwa (od wyjścia do wejścia)')\n",
        "plt.ylabel('Wielkość gradientu (log)')\n",
        "plt.title('Zanikający gradient w głębokich sieciach')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Gradient zanika eksponencjalnie!\")\n",
        "```\n",
        "\n",
        "**Rozwiązania:**\n",
        "- ReLU i warianty (LeakyReLU, ELU)\n",
        "- Batch Normalization\n",
        "- Skip Connections (ResNet)\n",
        "- Lepsza inicjalizacja wag (Xavier, He)\n",
        "\n",
        "---\n",
        "\n",
        "### Overfitting w głębokich sieciach\n",
        "\n",
        "**Overfitting:** Model \"zapamiętuje\" dane treningowe zamiast uczyć się ogólnych wzorców.\n",
        "\n",
        "```python\n",
        "# Demonstracja overfittingu\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dane z szumem\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X) + np.random.normal(0, 0.3, X.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Duża sieć bez regularyzacji → overfit\n",
        "# vs sieć z dropout/L2 → dobra generalizacja\n",
        "\n",
        "# Wykres learning curves pokazuje różnicę między train a test loss\n",
        "```\n",
        "\n",
        "**Techniki zapobiegania:**\n",
        "- **Dropout** - losowe wyłączanie neuronów\n",
        "- **L1/L2 Regularization** - karanie dużych wag\n",
        "- **Early stopping** - zatrzymanie gdy test loss przestaje maleć\n",
        "- **Data augmentation** - zwiększanie zbioru treningowego\n",
        "- **Batch Normalization** - efekt regularyzacyjny\n",
        "\n",
        "---\n",
        "\n",
        "### Ograniczenia w modelowaniu długoterminowych zależności\n",
        "\n",
        "Klasyczne RNN miały problemy z \"zapamiętywaniem\" informacji z odległej przeszłości w sekwencjach.\n",
        "\n",
        "```python\n",
        "# Problem vanilla RNN: informacja zanika po kilkunastu krokach\n",
        "class SimpleRNN:\n",
        "    def forward_sequence(self, sequence):\n",
        "        hidden = np.zeros(50)\n",
        "        for x in sequence:\n",
        "            hidden = np.tanh(np.dot(W_hh, hidden) + np.dot(W_xh, x))\n",
        "        return hidden\n",
        "\n",
        "# Po 20-30 krokach czasowych, pierwotna informacja praktycznie znika\n",
        "```\n",
        "\n",
        "**Rozwiązania:**\n",
        "- LSTM (Long Short-Term Memory)\n",
        "- GRU (Gated Recurrent Unit)\n",
        "- **Attention Mechanism** - bezpośredni dostęp do wcześniejszych stanów\n",
        "- **Transformers** - self-attention zamiast rekurencji\n",
        "\n",
        "---\n",
        "\n",
        "### Koszty obliczeniowe\n",
        "\n",
        "Trenowanie dużych modeli wymaga ogromnych zasobów.\n",
        "\n",
        "```python\n",
        "# Porównanie rozmiarów modeli\n",
        "models = ['AlexNet\\n2012', 'GPT-2\\n2019', 'GPT-3\\n2020', 'GPT-4\\n2023']\n",
        "params = [60, 1500, 175000, 1760000]  # Miliony parametrów\n",
        "costs = [10, 43, 4600, 100000]  # Tysiące USD\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(models, params, color='skyblue')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Parametry (miliony)')\n",
        "plt.title('Wzrost rozmiaru modeli')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, costs, color='lightcoral')\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Koszt treningu (tyś. USD)')\n",
        "plt.title('Wzrost kosztów')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Implikacje:**\n",
        "- Bariery wejścia dla małych graczy\n",
        "- Koszty środowiskowe (CO2)\n",
        "- Potrzeba efektywniejszych architektur\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Przełomowe innowacje techniczne\n",
        "\n",
        "### Dropout\n",
        "\n",
        "**Dropout** (Hinton, 2012) - losowe wyłączanie neuronów podczas treningu.\n",
        "\n",
        "```python\n",
        "# Dropout w praktyce\n",
        "class DropoutLayer:\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        self.rate = dropout_rate\n",
        "    \n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            mask = np.random.binomial(1, 1-self.rate, x.shape)\n",
        "            return x * mask / (1 - self.rate)  # Scaling\n",
        "        return x  # Bez dropout w inference\n",
        "```\n",
        "\n",
        "**Dlaczego działa:**\n",
        "- Ensemble effect - trenujemy wiele \"podsieci\"\n",
        "- Zmusza do robustności\n",
        "- Redukuje ko-adaptację neuronów\n",
        "\n",
        "---\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "**Batch Normalization** (2015) - normalizuje aktywacje w każdej warstwie.\n",
        "\n",
        "```python\n",
        "# Batch Normalization\n",
        "class BatchNorm:\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            mean = np.mean(x, axis=0)\n",
        "            var = np.var(x, axis=0)\n",
        "            x_norm = (x - mean) / np.sqrt(var + 1e-5)\n",
        "        else:\n",
        "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + 1e-5)\n",
        "        \n",
        "        # Scale and shift (learnable)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "```\n",
        "\n",
        "**Zalety:**\n",
        "- Przyspiesza trening (wyższy learning rate)\n",
        "- Stabilizuje trening\n",
        "- Efekt regularyzacyjny\n",
        "\n",
        "---\n",
        "\n",
        "### Skip Connections i Residual Learning\n",
        "\n",
        "**Skip Connections** (ResNet, 2015) - bezpośrednie połączenia między warstwami.\n",
        "\n",
        "```python\n",
        "# Residual block\n",
        "class ResidualBlock:\n",
        "    def forward(self, x):\n",
        "        identity = x  # Skip connection\n",
        "        \n",
        "        # F(x) - funkcja uczona przez warstwy\n",
        "        out = conv1(x)\n",
        "        out = relu(out)\n",
        "        out = conv2(out)\n",
        "        \n",
        "        # Dodanie skip connection\n",
        "        out = out + identity  # y = F(x) + x\n",
        "        out = relu(out)\n",
        "        return out\n",
        "```\n",
        "\n",
        "**Dlaczego działa:**\n",
        "- Gradient flow przez skip connections\n",
        "- Łatwo nauczyć się identity mapping (F=0)\n",
        "- Umożliwił sieci 100+ warstw\n",
        "\n",
        "**Przełom:** ResNet wygrał ImageNet 2015 z błędem 3.6%\n",
        "\n",
        "---\n",
        "\n",
        "### Attention Mechanism\n",
        "\n",
        "**Attention** (2014) - model sam decyduje, na które części wejścia patrzeć.\n",
        "\n",
        "```python\n",
        "# Uproszczony attention\n",
        "def attention(query, keys, values):\n",
        "    # 1. Oblicz similarity (Query · Key)\n",
        "    scores = np.dot(query, keys.T) / np.sqrt(keys.shape[-1])\n",
        "    \n",
        "    # 2. Softmax → rozkład prawdopodobieństwa\n",
        "    weights = softmax(scores)\n",
        "    \n",
        "    # 3. Ważona suma Values\n",
        "    output = np.dot(weights, values)\n",
        "    return output, weights\n",
        "```\n",
        "\n",
        "**Zalety:**\n",
        "- Długie zależności bez problemu\n",
        "- Przetwarzanie równoległe (vs RNN)\n",
        "- Interpretowalność (widzimy wagi uwagi)\n",
        "- Fundament Transformerów (GPT, BERT, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### Transfer Learning\n",
        "\n",
        "**Transfer Learning** - wykorzystanie wiedzy z jednego problemu do rozwiązania innego.\n",
        "\n",
        "```python\n",
        "# Transfer learning w praktyce\n",
        "# 1. Załaduj pre-trained model (np. ResNet na ImageNet)\n",
        "model = load_pretrained_model('resnet50')\n",
        "\n",
        "# 2. Zamroź wczesne warstwy\n",
        "for layer in model.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 3. Dodaj nowy classifier dla twojego zadania\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 4. Fine-tune na twoich danych\n",
        "model.fit(your_data, your_labels)\n",
        "```\n",
        "\n",
        "**Zalety:**\n",
        "- Mniej danych potrzebnych\n",
        "- Szybszy trening\n",
        "- Lepsza wydajność\n",
        "- Demokratyzacja AI\n",
        "\n",
        "**Strategie:**\n",
        "- Feature extraction (frozen weights)\n",
        "- Fine-tuning (aktualizacja wszystkich/niektórych warstw)\n",
        "- Progressive fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "## 1.4 Dlaczego potrzebujemy nowych architektur\n",
        "\n",
        "### Nowe dziedziny zastosowań\n",
        "\n",
        "**Medycyna:**\n",
        "- Diagnostyka obrazowa (CT, MRI)\n",
        "- Odkrywanie leków\n",
        "- Wymaga: interpretability, uncertainty estimation\n",
        "\n",
        "**Autonomous Vehicles:**\n",
        "- Percepcja otoczenia w czasie rzeczywistym\n",
        "- Wymaga: real-time, high reliability\n",
        "\n",
        "**Edge & Mobile:**\n",
        "- Smartfony, IoT, wearables\n",
        "- Wymaga: extreme efficiency, low latency\n",
        "\n",
        "**Multimodal AI:**\n",
        "- Vision + Language + Audio\n",
        "- Wymaga: unified architectures\n",
        "\n",
        "---\n",
        "\n",
        "### Rosnące wymagania\n",
        "\n",
        "```python\n",
        "# Trendy w wymaganiach (2015 = baseline)\n",
        "years = np.arange(2015, 2026)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(years, np.exp((years-2015)*0.3), label='Rozmiar danych')\n",
        "plt.plot(years, np.exp((years-2015)*0.25), label='Złożoność zadań')\n",
        "plt.plot(years, np.exp((years-2015)*0.2), label='Real-time requirements')\n",
        "plt.xlabel('Rok')\n",
        "plt.ylabel('Wzrost względem 2015')\n",
        "plt.title('Rosnące wymagania dla architektur AI')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Kierunki rozwoju\n",
        "\n",
        "**Efficiency & Scale:**\n",
        "- Sparse models (Mixture of Experts)\n",
        "- Quantization, pruning\n",
        "- Neural Architecture Search\n",
        "\n",
        "**Better Learning:**\n",
        "- Few-shot, zero-shot learning\n",
        "- Self-supervised learning\n",
        "- Continual learning\n",
        "\n",
        "**New Paradigms:**\n",
        "- State Space Models (Mamba, S4)\n",
        "- Graph Neural Networks\n",
        "- Geometric Deep Learning\n",
        "\n",
        "**Interpretability & Trust:**\n",
        "- Explainable AI (XAI)\n",
        "- Uncertainty quantification\n",
        "- Fairness & robustness\n",
        "\n",
        "---\n",
        "\n",
        "## Podsumowanie Modułu 1\n",
        "\n",
        "### Kluczowe punkty historyczne:\n",
        "- **Perceptron (1958)** - początek, ograniczony do problemów liniowych\n",
        "- **Backpropagation (1986)** - umożliwił wielowarstwowe sieci\n",
        "- **AlexNet (2012)** - przełom deep learning\n",
        "- **ResNet (2015)** - skip connections, sieci 100+ warstw\n",
        "\n",
        "### Główne problemy i rozwiązania:\n",
        "| Problem | Rozwiązanie |\n",
        "|---------|-------------|\n",
        "| Zanikający gradient | ReLU, Batch Norm, Skip Connections |\n",
        "| Overfitting | Dropout, regularyzacja, augmentacja |\n",
        "| Długie zależności | LSTM, GRU, Attention, Transformers |\n",
        "| Koszty obliczeniowe | Efektywne architektury, transfer learning |\n",
        "\n",
        "### Przełomowe innowacje:\n",
        "- ✅ **Dropout** - regularyzacja przez losowe wyłączanie\n",
        "- ✅ **Batch Normalization** - stabilizacja treningu\n",
        "- ✅ **Skip Connections** - głębokie sieci (ResNet)\n",
        "- ✅ **Attention** - fundament Transformerów\n",
        "- ✅ **Transfer Learning** - demokratyzacja AI\n",
        "\n",
        "### Współczesne wyzwania:\n",
        "- Efektywność (edge computing)\n",
        "- Multimodalność\n",
        "- Długie konteksty\n",
        "- Interpretability\n",
        "- Zrównoważony rozwój (Green AI)\n",
        "\n",
        "---\n",
        "\n",
        "## Co dalej?\n",
        "\n",
        "W **Modułu 2** zagłębimy się w konkretne rodziny architektur:\n",
        "- Sieci konwolucyjne (CNN → ResNet → EfficientNet → ConvNeXt)\n",
        "- Transformery i mechanizmy uwagi (ViT, BERT, GPT)\n",
        "- Architektury rekurencyjne i ich następcy\n",
        "- Podejścia hybrydowe\n",
        "\n",
        "Każda z tych rodzin rozwiązuje inne problemy i ma swoje zastosowania!\n",
        "\n",
        "---\n",
        "\n",
        "**Pytania do przemyślenia:**\n",
        "1. Które z przedstawionych innowacji uważasz za najbardziej przełomowe?\n",
        "2. Jakie nowe problemy mogą wymagać kolejnych przełomów?\n",
        "3. Dokąd zmierza rozwój architektur w następnej dekadzie?"
      ],
      "metadata": {
        "id": "TU7Fh3BbS89x"
      }
    }
  ]
}