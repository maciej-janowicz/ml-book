{"cells":[{"cell_type":"markdown","metadata":{"id":"069AQy_Kerno"},"source":["## PyTorch\n","\n","### PyTorch to otwarta biblioteka do uczenia maszynowego i obliczeń numerycznych.\n","### Została stworzona przez Facebook's AI Research lab (FAIR) i jest szeroko stosowana zarówno w przemyśle, jak i w badaniach naukowych.\n","### Obsługuje obliczenia na różnych platformach, w tym CPU, GPU i TPU.\n","### Zapewnia elastyczność i skalowalność, umożliwiając tworzenie złożonych modeli uczenia maszynowego.\n","### Posiada bogaty ekosystem narzędzi i bibliotek, takich jak TorchVision dla przetwarzania obrazów i TorchText dla przetwarzania języka naturalnego.\n","### Jest wykorzystywana w różnych dziedzinach, takich jak grafika komputerowa, przetwarzanie języka naturalnego i uczenie ze wzmocnieniem.\n","### Obsługuje wiele języków programowania, w tym Python i C++.\n"]},{"cell_type":"markdown","metadata":{"id":"X6tWOjgHfBDv"},"source":["### Klasyfikacja irysów\n","\n","#### Opis czynności:\n","\n","\n","1. Wczytujemy zbiór danych Iris i dzielimy go na zbiór treningowy i testowy.\n","2. Normalizujemy dane za pomocą StandardScaler.\n","3. Konwertujemy dane na tensory i tworzymy DataLoadery.\n","4. Definiujemy model klasyfikacji z trzema warstwami: dwie ukryte warstwy z funkcją aktywacji ReLU i warstwa wyjściowa.\n","5. Inicjalizujemy model, funkcję straty CrossEntropyLoss i optymalizator Adam.\n","6. Trenujemy model przez 100 epok.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15939,"status":"ok","timestamp":1733240407714,"user":{"displayName":"Maciej Janowicz","userId":"09531894793951009605"},"user_tz":-60},"id":"6_2rFG5VegYh","outputId":"89d1c92f-7f5d-498f-d8d6-6220eb217560"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dokładność dla danych testowych: 100.0%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Wczytanie danych\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Podział danych na zbiór treningowy i testowy\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalizacja danych\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Konwersja danych na tensory\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.long)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","# Tworzenie DataLoaderów\n","train_dataset = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n","test_dataset = TensorDataset(X_test, y_test)\n","test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n","\n","# Definicja modelu\n","class IrisClassifier(nn.Module):\n","    def __init__(self):\n","        super(IrisClassifier, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, 3)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Inicjalizacja modelu, funkcji straty i optymalizatora\n","model = IrisClassifier()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Trenowanie modelu\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    for X_batch, y_batch in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(X_batch)\n","        loss = criterion(outputs, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Ocena modelu\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for X_batch, y_batch in test_loader:\n","        outputs = model(X_batch)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += y_batch.size(0)\n","        correct += (predicted == y_batch).sum().item()\n","    print(f'Dokładność dla danych testowych: {100 * correct / total}%')\n"]},{"cell_type":"markdown","metadata":{"id":"TXbIB7Wifp3c"},"source":["### Klasyfikacja cyfr pisanych ręcznie\n","\n","#### Opis czynności\n","\n","\n","1.     Wczytujemy zbiór danych MNIST i przekształcamy dane na odpowiednią postać tensorową.\n","2.     Tworzymy DataLoadery dla zbiorów treningowego i testowego.\n","3.     Definiujemy model klasyfikacji z warstwą spłaszczającą, warstwą ukrytą z funkcją aktywacji ReLU i warstwą wyjściową.\n","4.    Inicjalizujemy model, funkcję straty CrossEntropyLoss i optymalizator Adam.\n","5.     Trenujemy model przez 10 epok.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211416,"status":"ok","timestamp":1733240619127,"user":{"displayName":"Maciej Janowicz","userId":"09531894793951009605"},"user_tz":-60},"id":"L46_Xr0KgrHP","outputId":"c9dae055-da39-4930-9305-74932308b42d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 41.9MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 1.20MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 9.37MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 2.18MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Dokładność dla danych testowych: 97.23%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Transformacje danych\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","# Wczytanie danych\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Tworzenie DataLoaderów\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Definicja modelu\n","class MNISTClassifier(nn.Module):\n","    def __init__(self):\n","        super(MNISTClassifier, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Inicjalizacja modelu, funkcji straty i optymalizatora\n","model = MNISTClassifier()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Trenowanie modelu\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for X_batch, y_batch in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(X_batch)\n","        loss = criterion(outputs, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Ocena modelu\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for X_batch, y_batch in test_loader:\n","        outputs = model(X_batch)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += y_batch.size(0)\n","        correct += (predicted == y_batch).sum().item()\n","    print(f'Dokładność dla danych testowych: {100 * correct / total}%')\n"]},{"cell_type":"markdown","metadata":{"id":"r_MuLkeIkCKx"},"source":["### Klasyfikacja obrazów odzieży\n","\n","\n","1.    Wczytujemy zbiór danych Fashion MNIST i przekształcamy dane na odpowiednią postać tensorową.\n","2.    Tworzymy DataLoadery dla zbiorów treningowego i testowego.\n","3.    Definiujemy model klasyfikacji z warstwą spłaszczającą, warstwą ukrytą z funkcją aktywacji ReLU i warstwą wyjściową.\n","4.    Inicjalizujemy model, funkcję straty CrossEntropyLoss i optymalizator Adam.\n","5.    Trenujemy model przez 10 epok.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"eq_6DwvWke56","outputId":"7d7f6804-4d40-4bcc-c07f-f6bd9d869943"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 26.4M/26.4M [00:01<00:00, 15.7MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 29.5k/29.5k [00:00<00:00, 272kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5.15k/5.15k [00:00<00:00, 13.3MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Dokładność dla danych testowych: 88.14%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Transformacje danych\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","# Wczytanie danych\n","train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Tworzenie DataLoaderów\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Definicja modelu\n","class FashionMNISTClassifier(nn.Module):\n","    def __init__(self):\n","        super(FashionMNISTClassifier, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Inicjalizacja modelu, funkcji straty i optymalizatora\n","model = FashionMNISTClassifier()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Trenowanie modelu\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for X_batch, y_batch in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(X_batch)\n","        loss = criterion(outputs, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Ocena modelu\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for X_batch, y_batch in test_loader:\n","        outputs = model(X_batch)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += y_batch.size(0)\n","        correct += (predicted == y_batch).sum().item()\n","    print(f'Dokładność dla danych testowych: {100 * correct / total}%')\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/r0JO8fIo6qfoN41AG2lR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}